{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofGvuAWfUAqn",
        "outputId": "16c8dacf-6ce7-42a2-98d2-eb5d53e3dd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'CH*': No such file or directory\n",
            "rm: cannot remove 'practice*': No such file or directory\n",
            "Cloning into 'KAIST_CH453_AI_chemistry'...\n",
            "remote: Enumerating objects: 1981, done.\u001b[K\n",
            "remote: Counting objects: 100% (1981/1981), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1971/1971), done.\u001b[K\n",
            "remote: Total 1981 (delta 14), reused 1910 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1981/1981), 5.56 MiB | 11.63 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2024.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (11.0.0)\n",
            "2024.03.6\n",
            "assignment_4  README.md  sample_data\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE!\n",
        "!rm -r CH*\n",
        "!rm -r assignment*\n",
        "!rm -r practice*\n",
        "!git clone https://github.com/oneoftwo/KAIST_CH453_AI_chemistry/\n",
        "!mv ./KAIST_CH453_AI_chemistry/assignments/assignment_4/ ./\n",
        "!rm -r KAIST_CH453*\n",
        "!pip install rdkit\n",
        "import rdkit\n",
        "print(rdkit.__version__)\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nNwlexcpTEv4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Descriptors import ExactMolWt\n",
        "from rdkit.Chem.Crippen import MolLogP\n",
        "from functools import reduce # optional\n",
        "from tqdm import tqdm # optional\n",
        "import numpy as np\n",
        "from rdkit import RDLogger\n",
        "lg = RDLogger.logger()\n",
        "lg.setLevel(RDLogger.CRITICAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQr58xcVnKWo"
      },
      "source": [
        "# **1. Define Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nQakh04AqwKI"
      },
      "outputs": [],
      "source": [
        "class MolDataset(Dataset):\n",
        "\n",
        "    def __init__(self, smi_list):\n",
        "        super().__init__()\n",
        "        self.smi_list = smi_list\n",
        "\n",
        "        self._set_c_to_i()\n",
        "        self._set_i_to_c()\n",
        "        self.vec_dim = self._get_num_char()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smi_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        return a dict of {\"input\": input, \"output\": output, \"length\", length},\n",
        "        where input is a long tensor of seq. encoded smiles,\n",
        "        output is a float tensor of corresponding logp value, and\n",
        "        length is a long tensor of \"a length of smiles\".\n",
        "        use self._encode_smi and self._get_logp.\n",
        "        '''\n",
        "        sample = dict()\n",
        "        smi = self.smi_list[idx]\n",
        "\n",
        "        input = self._encode_smi(smi)\n",
        "        logp = self._get_logp(smi)\n",
        "\n",
        "        sample = {\n",
        "                \"input\": torch.LongTensor(input),\n",
        "                \"length\": torch.LongTensor([len(smi) + 1]), ### +1 due to <EOS>\n",
        "                \"logp\": torch.Tensor(logp)\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "    def _set_c_to_i(self):\n",
        "        '''\n",
        "        Obtain c_to_i dictionary from smi_list.\n",
        "        We'll use the characters in self.smi_list, and auxiliary character 'X'.\n",
        "        '''\n",
        "        whole_char = ['X'] ##### auxiliary token, padding_value = 0\n",
        "        whole_char += list(reduce(lambda x, y: x | y, \\\n",
        "                [set(smi) for smi in self.smi_list]))\n",
        "        c_to_i = {c: i for i, c in enumerate(whole_char)}\n",
        "        self.c_to_i = c_to_i\n",
        "\n",
        "    def _set_i_to_c(self):\n",
        "        self.i_to_c = {v:k for k, v in self.c_to_i.items()}\n",
        "\n",
        "    def _get_c_to_i(self):\n",
        "        return self.c_to_i\n",
        "\n",
        "    def _get_i_to_c(self):\n",
        "        return self.i_to_c\n",
        "\n",
        "    def _encode_smi(self, smi):\n",
        "        return np.array([self.c_to_i[c] for c in smi + 'X'])\n",
        "\n",
        "    def _get_num_char(self):\n",
        "        return len(getattr(self, \"c_to_i\", dict()))\n",
        "\n",
        "    def _get_logp(self, smi):\n",
        "        '''\n",
        "        return a numpy array of logP of given smiles.\n",
        "        '''\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        return np.array([MolLogP(mol)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6n98V324S6A0"
      },
      "outputs": [],
      "source": [
        "def random_splitter(dataset, train_ratio, validation_ratio, test_ratio):\n",
        "    import random\n",
        "    import copy\n",
        "    assert train_ratio + validation_ratio + test_ratio == 1.0\n",
        "    N = len(dataset)\n",
        "    all_idx = list(range(N))\n",
        "    random.shuffle(all_idx)\n",
        "\n",
        "    train_idx = all_idx[:int(train_ratio * N)]\n",
        "    valid_idx = all_idx[int(train_ratio * N):int(validation_ratio * N) \\\n",
        "                        + int(train_ratio * N)]\n",
        "    test_idx = all_idx[int(validation_ratio * N) + int(train_ratio * N):]\n",
        "    train_dataset = copy.deepcopy(dataset)\n",
        "    valid_dataset = copy.deepcopy(dataset)\n",
        "    test_dataset  = copy.deepcopy(dataset)\n",
        "    train_dataset.smi_list = [dataset.smi_list[i] for i in train_idx]\n",
        "    valid_dataset.smi_list = [dataset.smi_list[i] for i in valid_idx]\n",
        "    test_dataset.smi_list =  [dataset.smi_list[i] for i in  test_idx]\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sEMossR3iv1p"
      },
      "outputs": [],
      "source": [
        "def sample_collate_fn(samples):\n",
        "    '''\n",
        "    Dataloader will make a list of samples with a len(samples) = batch_size.\n",
        "    Collate function should pad all the tensors in every sample at maximum size,\n",
        "    and stack them on a batch dimension.\n",
        "\n",
        "    Example)\n",
        "    if four tensors of shape (3, 7), (2, 7), (6, 7), (4, 7) is given,\n",
        "    collated tensor will have a shape of (4, 6, 7) where 4 is a batch size.\n",
        "    '''\n",
        "    inputs = pad_sequence([sample[\"input\"] for sample in samples], \\\n",
        "            batch_first=True, padding_value=0)\n",
        "    lengths = torch.cat([sample[\"length\"] for sample in samples], dim=0)\n",
        "    logps = torch.cat([sample[\"logp\"] for sample in samples], dim=0)\n",
        "\n",
        "    sample_batch = {\n",
        "            \"input\": inputs,\n",
        "            \"length\": lengths,\n",
        "            \"logp\": logps\n",
        "    }\n",
        "    return sample_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW15SofERZ_D",
        "outputId": "b580d059-cf19-4960-a27c-4e7dddcd597d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'X': 0, 'c': 1, 'C': 2, 'N': 3, 'O': 4, '1': 5}\n",
            "{0: 'X', 1: 'c', 2: 'C', 3: 'N', 4: 'O', 5: '1'}\n",
            "{'input': tensor([2, 4, 2, 2, 0]), 'length': tensor([5]), 'logp': tensor([0.6527])}\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "sample_smi_list = [\"c1ccccc1\", \"COCC\", \"CCCCCCCCCCCCN\"]\n",
        "\n",
        "sample_dataset = MolDataset(sample_smi_list)\n",
        "print(sample_dataset.c_to_i)\n",
        "print(sample_dataset.i_to_c)\n",
        "print(sample_dataset[1])\n",
        "print(sample_dataset._get_num_char())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiX7AAGsooRv"
      },
      "source": [
        "# **2. Define VAE Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xQGEBa9goVuI"
      },
      "outputs": [],
      "source": [
        "class VariationalAutoEncoder(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_char,\n",
        "            n_hidden,\n",
        "            n_rnn_layer\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_char = n_char\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_rnn_layer = n_rnn_layer\n",
        "\n",
        "        self.sos = nn.Parameter(torch.ones(1, 1, n_hidden))\n",
        "        self.sos.requires_grad = False\n",
        "\n",
        "        self.embedding = nn.Embedding(n_char, n_hidden)\n",
        "        self.encoder = nn.GRU(input_size=n_hidden, hidden_size=n_hidden, \\\n",
        "                          num_layers=n_rnn_layer, batch_first=True)\n",
        "        self.decoder = nn.GRU(input_size=n_hidden, hidden_size=n_hidden, \\\n",
        "                          num_layers=n_rnn_layer, batch_first=True)\n",
        "        self.mu_layer = nn.Linear(n_hidden, n_hidden)\n",
        "        self.logvar_layer = nn.Linear(n_hidden, n_hidden)\n",
        "        self.final_layer = nn.Linear(n_hidden, n_char)\n",
        "\n",
        "        self.rec_loss_fn = nn.CrossEntropyLoss()\n",
        "        self.vae_loss_fn = self.vae_loss\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.rand_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def vae_loss(self, mu, logvar):\n",
        "        return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    def forward(self, x, length):\n",
        "        '''\n",
        "        x (torch.Tensor): [B, L]\n",
        "        '''\n",
        "        B = x.shape[0] # batch size\n",
        "\n",
        "        # 1. Embedding x[B L] -> h[B L F]\n",
        "        h = self.embedding(x) # [B L F]\n",
        "\n",
        "        # 2. Encoder h[B L F] -> enc[B L F] -> z[B F]\n",
        "        enc, _ = self.encoder(h) # [B L F]\n",
        "        z = torch.stack([enc[i, length[i] - 1, :] for i in range(B)], dim=0) # [B F]\n",
        "\n",
        "        # 3. Get mu and logvar from z and reparameterize\n",
        "        mu = self.mu_layer(z)\n",
        "        logvar = self.logvar_layer(z)\n",
        "        rep_z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # 4. Decoder z[B F], dec_in[B 1+L F] -> dec[B 1+L F]\n",
        "        sos_vec = self.sos.repeat(B, 1, 1) # [B 1 F]\n",
        "        dec_in = torch.cat([sos_vec, h], dim=1) # [B 1+L F]\n",
        "        dec, _ = self.decoder(dec_in, rep_z.unsqueeze(0).repeat(self.n_rnn_layer, 1, 1))\n",
        "\n",
        "        # 5. Predict the probability of character dec[B 1+L F] -> dec_final[B 1+L N_CHAR]\n",
        "        dec_final = self.final_layer(dec) # [B 1+L N_CHAR]\n",
        "\n",
        "        # 6. Calculate loss\n",
        "        rec_losses = torch.stack(\n",
        "            [self.rec_loss_fn(dec_final[i, :length[i], :], x[i, :length[i]]) \\\n",
        "             / length[i] for i in range(B)], dim=0\n",
        "        ) # length due to <SOS>\n",
        "        rec_loss = rec_losses.mean()\n",
        "        vae_loss = self.vae_loss_fn(mu, logvar)\n",
        "\n",
        "        loss = rec_loss + vae_loss\n",
        "        return loss, rec_loss, vae_loss\n",
        "\n",
        "    def generate(self, max_length, batch_size=1):\n",
        "        z = torch.randn(self.n_rnn_layer, batch_size, self.n_hidden).to(self.sos.device)\n",
        "        dec_in = self.sos.repeat(batch_size, 1, 1) # [B 1 F]\n",
        "\n",
        "        gen_smis = []\n",
        "        i = 0\n",
        "        while i < max_length:\n",
        "            dec_out, z = self.decoder(dec_in, z)\n",
        "            dec_final = nn.functional.softmax(self.final_layer(dec_out), dim=-1)\n",
        "            index = torch.argmax(dec_final)\n",
        "\n",
        "            if index == 0: break # <eos>\n",
        "            gen_smis.append(int(index))\n",
        "            dec_in = self.embedding(index.view(1, -1).long())\n",
        "            i += 1\n",
        "        if i == max_length: return [] # generation failed\n",
        "        return gen_smis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYgROfDsan7r",
        "outputId": "428c755f-6f08-4bd0-f3e1-e19945a3244e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': tensor([[1, 5, 1, 1, 1, 1, 1, 5, 0, 0, 0, 0, 0, 0],\n",
            "        [2, 4, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0]]), 'length': tensor([ 9,  5, 14]), 'logp': tensor([1.6866, 0.6527, 3.8660])}\n",
            "tensor(0.2547, grad_fn=<AddBackward0>)\n",
            "tensor(0.2255, grad_fn=<MeanBackward0>)\n",
            "tensor(0.0292, grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "dataloader = DataLoader(sample_dataset, batch_size=3, shuffle=False, collate_fn=sample_collate_fn)\n",
        "dataiter = iter(dataloader)\n",
        "batch = next(dataiter)\n",
        "print(batch)\n",
        "\n",
        "sample_model = VariationalAutoEncoder(6, 64, 1)\n",
        "loss, rec_loss, vae_loss = sample_model(batch[\"input\"], batch[\"length\"])\n",
        "print(loss)\n",
        "print(rec_loss)\n",
        "print(vae_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G-GrQMjT1nD"
      },
      "source": [
        "# **3. Hyperparameter Settings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qZZF_zOWT8IE"
      },
      "outputs": [],
      "source": [
        "########## DO NOT CHANGE ##########\n",
        "NUM_EPOCH = 25\n",
        "LR = 1e-3\n",
        "N_HIDDEN = 256\n",
        "N_RNN_LAYER = 2\n",
        "BATCH_SIZE = 256\n",
        "DATA_DIR = \"./assignment_4/PubChem_30K.txt\"\n",
        "####################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lIpq15QULGr"
      },
      "source": [
        "# **4. Build the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxhEk-grUP1B",
        "outputId": "ca7cfe2c-c2d9-4ccc-9c2e-3adfdf52d877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20667\n",
            "{'input': tensor([57, 45, 14, 57,  6, 54, 43, 10, 15, 10, 10, 10, 14, 45,  6, 45, 10, 13,\n",
            "        10, 10, 10, 10, 10, 13, 43, 10, 10, 15,  0]), 'length': tensor([29]), 'logp': tensor([3.6946])}\n"
          ]
        }
      ],
      "source": [
        "with open(DATA_DIR, 'r') as f:\n",
        "    smi_list = [l.strip().split()[1] for l in f.readlines()]\n",
        "    smi_list = [x for x in smi_list if len(x) > 10 and len(x)< 60]\n",
        "\n",
        "dataset = MolDataset(smi_list)\n",
        "train_dataset, valid_dataset, test_dataset = \\\n",
        "        random_splitter(dataset, 0.8, 0.2, 0.0)\n",
        "\n",
        "N_CHAR = dataset._get_num_char()\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCgQkixaUUal"
      },
      "source": [
        "# **5. Build the DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cpiW3R9MVR4m"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=sample_collate_fn)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=sample_collate_fn)\n",
        "tr_N = len(train_dataset)\n",
        "va_N = len(valid_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4-15VkBVoKQ"
      },
      "source": [
        "# **6. Set Model and Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l-Hjz9cRVsQ2"
      },
      "outputs": [],
      "source": [
        "model = VariationalAutoEncoder(\n",
        "            N_CHAR,\n",
        "            N_HIDDEN,\n",
        "            N_RNN_LAYER\n",
        "        )\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCnSgK0KV0qZ"
      },
      "source": [
        "# **7. Train with Mini-batches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N6hNPx9V6d_",
        "outputId": "9c26db1a-b711-4058-bc24-9dac6c357fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "81it [00:28,  2.87it/s]\n",
            "21it [00:04,  4.56it/s]                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t1th EPOCH --- TRAIN LOSS: 0.0621 || VALIDATION LOSS: 0.0446 || BEST EPOCH: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 71%|███████▏  | 57/80 [00:17<00:06,  3.67it/s]"
          ]
        }
      ],
      "source": [
        "save_dir = \"./save/\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "\n",
        "train_loss_history, valid_loss_history = [], []\n",
        "best_loss = 1e6\n",
        "for i in range(1, NUM_EPOCH + 1):\n",
        "\n",
        "    model.train()\n",
        "    train_batch_losses = []\n",
        "    for batch_idx, batch in tqdm(enumerate(train_dataloader), total=tr_N // BATCH_SIZE):\n",
        "        x_batch = batch[\"input\"].long().cuda()\n",
        "        l_batch = batch[\"length\"].long().cuda()\n",
        "\n",
        "        loss, rec_loss, vae_loss = model(x_batch, l_batch)\n",
        "        train_batch_losses.append(loss.data.cpu().numpy())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_batch_losses = []\n",
        "        for batch_idx, batch in tqdm(enumerate(valid_dataloader), total=va_N // BATCH_SIZE):\n",
        "            x_batch = batch[\"input\"].long().cuda()\n",
        "            l_batch = batch[\"length\"].long().cuda()\n",
        "\n",
        "            loss, rec_loss, vae_loss = model(x_batch, l_batch)\n",
        "            valid_batch_losses.append(loss.data.cpu().numpy())\n",
        "\n",
        "    train_avg_loss = np.mean(np.array(train_batch_losses))\n",
        "    valid_avg_loss = np.mean(np.array(valid_batch_losses))\n",
        "    train_loss_history.append(train_avg_loss)\n",
        "    valid_loss_history.append(valid_avg_loss)\n",
        "\n",
        "    if valid_avg_loss < best_loss:\n",
        "        best_epoch = i\n",
        "        best_loss = valid_avg_loss\n",
        "\n",
        "    print(f\"\\t{i}th EPOCH --- TRAIN LOSS: {train_avg_loss:.4f} || VALIDATION LOSS: {valid_avg_loss:.4f} || BEST EPOCH: {best_epoch}\", flush=True)\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, f\"save_{i}.pt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60JYx5BPWYVR"
      },
      "source": [
        "# **8. Plot the Loss Histories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaGuPAdiWcCM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x_axis = np.arange(NUM_EPOCH)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x_axis, train_loss_history, label='train loss')\n",
        "ax.plot(x_axis, valid_loss_history, label='validation loss')\n",
        "ax.set_xlabel('Num epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Loss History')\n",
        "ax.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGYehu2SWmwq"
      },
      "source": [
        "# **9. Generate SMILES with VAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQWYe-bmWqB9"
      },
      "outputs": [],
      "source": [
        "def index_list_to_smiles(ind_list, i_to_c):\n",
        "    return \"\".join([i_to_c[x] for x in ind_list])\n",
        "\n",
        "\n",
        "save_state_dict = torch.load(os.path.join(save_dir, f\"save_{best_epoch}.pt\"))\n",
        "model.load_state_dict(save_state_dict)\n",
        "model.eval()\n",
        "\n",
        "max_length = 64\n",
        "num_sample = 200\n",
        "i_to_c = dataset._get_i_to_c()\n",
        "\n",
        "gen_smis = []\n",
        "for _ in tqdm(range(num_sample), total=num_sample):\n",
        "    gen_smi = model.generate(max_length)\n",
        "    gen_smis.append(index_list_to_smiles(gen_smi, i_to_c))\n",
        "\n",
        "print(gen_smis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfuzoWjNZHeZ"
      },
      "outputs": [],
      "source": [
        "# code for plotting molecules (just for sanitiy check)\n",
        "\n",
        "print(gen_smis)\n",
        "\n",
        "\n",
        "filtered_gen_smis = filter_smiles(gen_smis)[0]\n",
        "print(\"Number of valid SMILES:\", len(filtered_gen_smis))\n",
        "mols = [Chem.MolFromSmiles(smi) for smi in gen_smis][:10]\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from IPython.display import display\n",
        "\n",
        "mol_list = [Chem.MolFromSmiles(smiles) for smiles in gen_smis]\n",
        "image_size = (150, 150)  # Adjust the dimensions as needed\n",
        "\n",
        "# Plot and display the molecules with the specified size\n",
        "for mol in mol_list[:10]:\n",
        "    try:\n",
        "        display(Draw.MolToImage(mol, size=image_size))\n",
        "    except:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UgbNa5Lu8W3"
      },
      "outputs": [],
      "source": [
        "from rdkit.Chem import Draw, AllChem\n",
        "from rdkit.Chem.Draw import rdMolDraw2D\n",
        "from rdkit.Chem.Draw.MolDrawing import DrawingOptions\n",
        "from IPython.display import SVG\n",
        "\n",
        "def filter_smiles(smi_list, ref_smi_list=None):\n",
        "\n",
        "    # 1. Valid smiles?\n",
        "    valid_smi_list = []\n",
        "    for smi in smi_list:\n",
        "        if len(smi) == 0: continue\n",
        "        try:\n",
        "            mol = Chem.MolFromSmiles(smi)\n",
        "            assert mol is not None\n",
        "        except:\n",
        "            continue\n",
        "        valid_smi_list.append(smi)\n",
        "\n",
        "    # 2. Unique smiles?\n",
        "    unique_smi_list = []\n",
        "    for smi in valid_smi_list:\n",
        "        pass\n",
        "        ## Implement Here ## TODO\n",
        "\n",
        "    # 3. Novel smiles?\n",
        "    novel_smi_list = []\n",
        "    for smi in unique_smi_list:\n",
        "        pass\n",
        "        ## Implement Here ##\n",
        "\n",
        "    return valid_smi_list, unique_smi_list, novel_smi_list\n",
        "\n",
        "filtered_gen_smis = filter_smiles(gen_smis)[0]\n",
        "print(\"Number of valid SMILES:\", len(filtered_gen_smis))\n",
        "mols = [Chem.MolFromSmiles(smi) for smi in filtered_gen_smis][:10]\n",
        "\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "from IPython.display import display\n",
        "image_size = (150, 150)  # Adjust the dimensions as needed\n",
        "for mol in mols:\n",
        "    try:\n",
        "        display(Draw.MolToImage(mol, size=image_size))\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36LsQACGYzjs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}